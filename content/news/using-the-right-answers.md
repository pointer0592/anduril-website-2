---
title: Are You Using the Right Answers?
isLive: True
archived: false
published: 2020-11-12
description: Incorporating data analytics into workflow
lead: Artificial intelligence generates reams of data, but generating results means you need to separate wheat from chaff and focus on metrics that matter.
isMockup: false
coverimg: chart-sheet
smallimg: chart-sheet
imginsert: ''
insertheader: ''
insertimagesource: ''
gradient: dark
alt: chart
citation:
  label: ''
  link: ''
includebottom: false
author: 
  name: Jonathan Neitzell
  image: jonathan-neitzell
  linkedin: https://www.linkedin.com/in/jon-neitzell-1ba3172/
  title: Founder, Managing Partner
  company: Anduril Partners
authorTwo:
  name: ''
  image: ''
  linkedin: ''
  title: ''
  company: ''
tags: 
  - Data
  - Tech
  - KPIs
category: Blog Post
pdf: 'Using the Right Answers'
---

The growth in available data and potential from analytics and artificial intelligence (AI) technology increases both the opportunity and the challenge for asset managers, investor relations professionals, and corporate management teams. Analytics can influence corporate decisions in product management, capital allocation, and how equity shareholders impact share pricing. The size and scope strains comprehension:

- Approximately 20 billion Internet of Things devices are now online. By 2025, the number is expected to rise to 75 billion devices.
- There will be 4.8 billion internet users by 2022, up from 3.4 billion in 2017. 80 percent of data will be unstructured by 2025.
- More stored data has been created in the last two years than in the history of mankind prior to that point.
- Financial services firms are increasingly using this data to predict business model outcomes and set equity prices.

We continue to hear these statistics, but our eyes often gloss over given the challenge of understanding the disciplines required to integrate all this data.
We need five skills and tools to unlock the value of this data and use it our advantage:

1. Business knowledge of where value is created for the end customer.
2. The devices and sources of data and their biases.
3. Statistical and mathematical approaches to calculating what is known, and properly de.risking what is not.
4. Technology software and architecture requirements.
5. Cultural and organizational awareness and mutual respect for blending those respective skills into a tangible workflow. 
   
Thankfully, just as we saw with public cloud adoption, new "no code" tools and services are becoming available to make the scale and transparency of technology magic available to the business user who understands the core value proposition.
The Massachusetts Institute of Technology calls the knowledge made possible by this technology "shared intelligence" Early adopters have the opportunity to separate from the pack as Amazon did with its public cloud computing service and successful e-commerce businesses. The COVID-19 pandemic creates a further imperative to take action on these types of opportunities.

### The Importance of Storytelling

Even with advanced technology that can synthesize and deliver data in actionable formats, further context and understanding is still needed.
As Nobel Prize winners Danny Kahneman and Amos Tversky, were quoted as saying in the book,"The Undoing Project," by Michael Lewis, 
>"No one ever made a decision because of a number. They need a story."

Call it the "human experience side" of AI if you like.

To ground us in reality and reasonable expectations, it helps to reflect on the early pioneers of behavioral economics, which is the study of psychology as it relates to the economic decision-making processes of individuals and institutions.
What researchers in this area found was fascinating. During studies of even highly educated, scientifically disciplined doctorates in medicine and statistics, Tversky and Kahneman found most people walk around with mental heuristics (habits) including availability, representativeness, and anchoring. Their research indicated we are prone to using recent availability or representativeness of personal experience to extrapolate probability, and our expectations can be anchored by the order in which we receive information--a humbling and troubling proposition.
Their groundbreaking findings were published in a paper, "Judgment Under Uncertainty: Heuristics and Biases," in 1974. Today, coupled with the birth and global domination of software, we increasingly look to technology for answers to protect us from misjudgment.
However, there is a bit of humor here. As the proliferation of technology solutions shown in the chart, "Data and the AI Landscape" on pages 24-25 further affirms, we need context and process to make the path realistic. Illustrating that resource without calibration is nearly useless, Samuel Taylor Coleridge, in his famous poem, "The Rime of the Ancient Mariner", writes:

>"Water, water, everywhere, Nor any drop to drink"

### Testing the Experts

As Kahneman and Tversky found among their highly educated audience decades ago, being surrounded by numbers is not relevant if it"s not digestible in our daily workflow. To underline this point, Paul Slovic, a psychologist and a peer of Nobel laureate Daniel Kahneman, decided to evaluate the effect of information on decision-making. He gathered a group of professional gamblers and tested them with horse races over four rounds.
Slovic told them the test would consist of predicting 40 horse races in four consecutive rounds. In the first round, each gambler was given five pieces of information about each horse. One might believe years of jockey experience was a key performance indicator (KPI); another might want horse top speed; and so on. (Industry examples of these types of KPI calculations are shown in the chart, "Business KPIs: A Universal Language," on this page.)
In addition to imageking winners, the experts were asked to indicate their level of confidence in their choice. In the first round with five pieces of information, they proved to be 17 percent accurate, substantially better than the 10 percent calculated chance prior to receiving their information. Their confidence was cited at 19 percent, relatively in line with the outcome.
They were then given 10 pieces of information in the second round and so on until they received 40 pieces of information in the final round. Interestingly, while their predictive ability flatlined at the 17% accuracy level, their confidence continued to rise with the additional information to expect a 34% hit rate!
This has significant ramifications for our ability to use raw information often driven by fear of missing out (FOMO) and untested assumptions. Unless we have disciplined consistent process with feedback loops, we risk being guilty of simply cherry-imageking data to enhance our confirmation bias--leaving us potentially wrong and extra confident about it.

### Can You OODA?

Thankfully for those with humility and a desire to drive consistent outperformance, the concept of feedback loops has become increasingly prevalent to digest and filter the mountain of data resources from noise into insight.
The U.S. military recognized realty is in a constant state of change (often in response to our own actions) and that excellence in process may be one of the few sustainable areas of persistent advantage. Based on this realization, strategy tactician John Boyd created a very straightforward framework called the OODA loop, which stands for Observe, Orient, Decide, and Act.
Now considered a foundational doctrine, it suggests that regardless of the backdrop, whomever is able to accurately observe (ingest data), orient (solve for KPIs), decide (designate the primary objective), and act (ability to execute) these steps and proceed back to the first step to observe success or failure of previous effort, will emerge victorious. To summarize, teams cycling well through this framework will win while opponents are choking on the noise and confusion of exponential information growth. Process matters!

### Applying Process in Financial Workflow

How can we integrate qualitative (human experience) and quantitative inputs into "shared intelligence?" The chart, "The Fusion of Data, Discipline, and Technology," on this page demonstrates how an asset management group might add specificity to an OODA loop concept, driving decisions on which allocations to purchase, how they integrate as a portfolio, and how they might assess attribution and error rate per input function step " a historical "holy grail" challenge.
On the left we have inputs such as SEC filings, internal or external analysis, industry relationships, and qualitative experiences. In the next column there are functions within systems that will change through manual or automated updates to reflect the changing reality of the world around us.

### Data and AI Landscape 2020

These input names may change based on the business model, but for financial services groups, this drives top and bottom line changes to forward estimates, and areas we believe to be operational key performance indicators to the asset related business model.
This is then reviewed based on portfolio risk parameters that may be as simple as a gut feel (how most business is actually done) or as complex as mathematical factor models. These steps culminate in a buy or sell decision, and then the forward performance of the asset begins to show actual outcomes.
If our effort has been recorded, now the magic begins--we can check our initial assumptions against actuals and run feedback statistics, error rates, and increasingly complex machine learning on this real-time and growing resource of training data and intellectual property. This allows data and institutional learning to become a tangible asset!

### Peeling Back the Veil

n a moment of stark honesty, most organizations will admit they have never actually drawn out their decision process, and the few that have will tell us with some flowchart they have a process. However, if the inputs are not touching software and creating a time series of quantified changes, the effort is incredibly prone to narrative shift, hindsight bias, and lack of objectivity. Consequently, the ability for feedback loops or incremental learning will be severely compromised.
It has been said, if software is eating the world, models will run the world. For those humble, confident, and willing to be held accountable, the tailwinds of technology can harness this tremendous potential in transparency, scale, and continued improvement on behalf of your stakeholders.

### Turning Questions into Predictions

One of the largest shifts we are likely to see in team discussions during the next five years is toward analytics and data-influenced decisions. To do this, we must take our qualitative, thematic questions and turn them into key performance indicators " hypotheses which can be quantified, tested, and predicted. This process entails integrating the personal experiences of business users and operators and attaching their primary metrics to data consistently available.
For the financial industry, analysts might answer questions about a company"s equity value by inferring revenue growth based on KPIs such as new customer growth, average spend per transaction, share of industry sales, and cohorts changing purchasing locations between physical and virtual storefronts. These may be seen within transaction records, email receipts, web traffic, or natural language processing queries of customer social media comments.
These discussions are often the same across corporate, private equity, and public equity uses, making a focus on defining, tracking, and predicting KPIs an increasingly universal language. Corporate intelligence and investor relations groups are likely to be a vital bridge between planning for resource allocation and explaining these key components to stakeholders. Do you have the process to "Observe, Orient, Decide, and Act" with this secular wave?

### ESG as a Use Case Example

Environmental, social, and governance (ESG) and sustainable investment is a rising focus across the asset management and capital allocation communities, but what does that really mean? We are early in this journey of quantifying many qualitative efforts, and this is an example of a where a realistic framework might coalesce.
Standard-setting bodies such as MSCI are issuing ranking systems such as risk factors or bond ratings in specific categories. They are increasingly joined by broker dealers and boutique research companies to create a more consistent and transparent framework for these metrics.
How do shareholders and corporates aggregate all these views and see relative peer group rankings? There will be an increasing number of ways to address this, but the diagram,  "A "New Lens" ESG Scorecard," on page 26 shows an aggregation platform with a single dashboard that allows for granularity in setting the exact criteria each organization wants to use for individual scores for each category of ESG. It then rolls those inputs into a total firm ranked score. This methodology provides consistent, scalable, real time, and transparent process to stakeholders.
Corporations and large foundation allocators can also use this with ownership data to see what the ESG scores are for each aggregated investment fund and see which institutions are really putting their money where their claimed priorities are.

### What is Driving the Stock?

Here we are going to tackle one of the holy grail interest points across communities--why a stock is trading where it is!
We are familiar with valuation factors such as EBITDA, Return on Invested Capital, and how important positive Wall Street "buy" rankings might be.
For those unfamiliar with risk model factors from Barra or Axioma, these are basically mathematically calculated relationships to certain thematic styles such as growth, value, leverage, size and others.
One of the easiest ways to understand this is to think of it like nutrition labels. The impact of a food (or asset) really depends on the build of macro nutrients. Using factor analysis, we can decompose what is driving asset pricing similar to how we can deconstruct a soup into nutrition macros such as protein, carbohydrates, and fats.
In the chart, "Factor and Fundamental KPI Example," on page 27 a company is compared to a group of its peers. While the stock was down 819 percent in this example, style factors negatively impacted its peer group by 25.93 percent, and the company actually recovered 17 percent due to company-specific tailwinds. Fundamentally, we can see that EBITDA margins and growth have an 84 percent correlation to stock price, demonstrating explicitly what shareholders care about most, with powerful ramifications for capital allocation decisions.
Think of how powerful this is to know when meeting with management or shareholders and this can be explicitly and empirically answered, along with exact correlation of whether the stock has been most influenced by top[-line growth or margin expansion. This adds tremendous granularity when coupled with equity owner investment discipline criteria or management capital allocation planning for M&A, buyback, or dividend policy.

### Water, Water Everywhere, Yet...

We have discussed use cases in several components in a decision workflow, but there are many, and everyone weights them differently for different durations. How do we blend all these inputs of different types into a system with feedback loops?
Using financial services as an example in the diagram, "What to Do With All the KPIs," this "Tower of Babel" of decision inputs will not scale using Excel and PowerPoint. The difficulty is that data comes from different sources, speaking languages includ.ing near-term and long-term fundamental business model KPIs, technology requirements, math and data science integrity, historical and peer valuation, and risk factor influences.
There are several critical challenges here:

1. These inputs are like trying to compare apples, oranges, and pears.
2. Per our example on the horse races, academic research shows after a certain level of inputs, analysts flatline their predictive ability (overwhelmed).
3. There are almost no transparent or consistent feedback loops without a software system for tracking and monitoring. We love to talk about ML/AI opportunities, but those don"t exist until one has the data in a system with feedback loops.

### Bringing it Home

For those interested in taking tangible steps to begin this journey, there are growing options to leverage technology and qualitative seasoned business acumen within process-driven software. This software integrates corporate intelligence and financial workflow decisions with inputs from fundamental business model views, statistical probabilities, real time nowcast data, internal analysis, and risk management into proprietary internal expected outcomes.
While this can be developed from scratch internally, increasingly these capabilities have already been developed from vertically focused vendors. In the view developed for the asset management diagram, "Single Pane of Glass," this allows nearly real-time integration of both existing and emerging priorities like ESG scores right into category rankings from classic financial workflow like demand-prediction, valuation, relative growth, profitability, quality and sentiment. This example rolls up into proprietary rules-based rankings scalable across all global assets consistently.

### Upgrade Your Roadmap

In conclusion, this should be a great conversation starter for asset managers, investor relations professionals, and corporate management teams--how are you ranking and prioritizing your core key performance indicators and decision processes? If this is only being done through lip service, there are bold new tools available for your transition to a process-driven approach.

### About the author

Jonathan Neitzell is Founder and Managing Partner at Anduril Partners, an investment and advisory firm focused on the application of data-driven processes
